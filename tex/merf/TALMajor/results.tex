In this section we compare \framework implementations of 
the narrator chain, temporal entity, and genealogy entity extration 
tasks to the task specific techniques proposed to solve them
in ANGE~\cite{ZaMaFlairs2012HadithBio}, 
ATEEMA~\cite{ZaMa2012IJCLATime},  and
GENTREE~\cite{ZaMaHaCicling2012Entity}, respectively. 
We also compare a \framework number normalization task to 
a task specific implementation. 
In the online appendix%
~\footnote{available at ~\url{-ommited-for-anonymity-}}%http://webfea.fea.aub.edu.lb/fadi/pdfs/merfappendix.pdf
, we report on eight additional \framework case studies.

Table~\ref{tab:results} reports the development time,
extraction runtime, recall and precision 
of the output MRE tags, 
the size of the task in lines of code or in number of \framework rules, 
for both the standalone task specific and the \framework implementations. 
%Table~\ref{tab:mbfer} reports the accuracy of the output MBF tags and the user-defined relation 
%construction in \framework for each task.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[tb!]
  \centering
\caption{\framework compared to task specific applications.}
  \resizebox{0.85\columnwidth}{!}{
    \begin{tabular}{l|l|l|ll|l}
     \toprule
     \multirow{2}{*}{Task} & Development& Run & \multicolumn{2}{c|}{Accuracy} & \multirow{2}{*}{Ease of Composition}\\
     & time & time(s) & Recall & \multicolumn{1}{c|}{Precision} & \\
    \midrule
    ANGE~\cite{ZaMaFlairs2012HadithBio}
    & 2 months & 1.79 & 0.99 & 0.99 & 3000+ lines of code\\
    \framework & 3 hours & 7.24 & 0.99 & 0.93  & 8 MBFs and 4 MREs\\
    \midrule
    ATEEMA~\cite{ZaMa2012IJCLATime}
    & 1.5 months & 2.53 & 0.88  & 0.89  & 1000+ lines of code  \\
    \framework & 3 hours & 3.14 & 0.91  & 0.81  & 3 MBFs and 2 MREs\\
    \midrule
    Genealogy tree~\cite{ZaMaHaCicling2012Entity} 
    & 3 weeks & 0.74 & 0.96 & 0.98 & 3000+ lines of code\\
    \framework & 4 hours & 2.28 & 0.84 & 0.93  & 3 MBFs and 3 MREs\\
    \midrule
    NUMNORM & 1 week & 0.32 & 0.91 & 0.93  & 500 lines of code\\
    \framework & 1 hour & 1.53 & 0.91 & 0.90 & 3 MBFs/1 MRE/57 lines\\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:results}%
\end{table}%

%Recall refers to the fraction of the entities correctly detected against 
%the total number of entities available. 
%Precision refers to the fraction of correctly detected entities against the 
%total number of extracted entities. 
%Intuitively, precision denotes whether the system generated false positives.

For the temporal and number normalization cases, 
we evaluated the techniques against arbitrary text from issues of 
the Lebanese Assafir and Al-Akhbar newspapers
\footnote{available at \url{http://www.assafir.com} and 
\url{http://www.al-akhbar.com}.}. 
For the narrator chain case, we used
Musnad Ahmad, a hadith book, for evaluation. 
For the genealogical tree extraction we used 
an extract from the Genesis biblical text.

%\subsection{Comparison}
Table~\ref{tab:results} shows that \framework has a clear advantage over 
task specific techniques in the effort required to develop the application at 
a reasonable cost in terms of accuracy and run time. 
Developers needed three hours, three hours, four hours, and one hour 
to develop the narrator chain, temporal entity, genealogy, and number 
normalization case studies using \framework, respectively. 
However, the developers of ANGE, ATEEMA, GENTREE, and 
NUMNORM needed two months, one and a half months, 
three weeks, and one week, respectively. 
\framework needed eight MBFs and four MREs for narrator chain, 
three MBFs and 2 MREs for temporal entity, three MBFs and three MREs for 
genealogy, and three MBFs, one MRE, and 57 lines of code actions for the number normalization tasks. 
However, ANGE, ATEEMA, GENTREE, and NUMNORM required 
3,000+, 1,000+, 3,000+, and 500 lines of code, respectively.

\framework required reasonably more runtime than the task specific 
implementations and reported acceptable and 
slightly less precision metrics with around
the same recall.
%a runtime of 7.29, 1.53, 3.14, and 2.28 seconds for 
%the narrator chain, temporal, genealogy, and number normalization tasks, respectively. 
%However, ANGE, ATEEMA, GENTREE, and NUMNORM required 1.79, 2.53, 0.74, and 0.32 seconds, respectively. 
%\framework required a runtime of 7.29, 1.53, 3.14, and 2.28 seconds for 
%the narrator chain, temporal, genealogy, and number normalization tasks, respectively. 
%However, ANGE, ATEEMA, GENTREE, and NUMNORM required 1.79, 2.53, 0.74, and 0.32 seconds, respectively. 
%In narrator chain, \framework scored 0.99\% recall and 0.93\% precision while ANGE scored 0.99\% recall and 0.99\% precision. 
%In temporal entity, \framework scored 0.91\% recall and 0.81\% precision while ATEEMA scored 0.88\% recall and 0.89\% precision. 
%In genealogy, \framework scored 0.84\% recall and 0.93\% precision while GENTREE scored 0.96\% recall and 0.98\% precision. 
%In number normalization, \framework scored 0.91\% recall and 0.90\% precision while NUMNORM scored 0.91\% recall and 0.93\% precision.

\setcode{utf8}
\setarab
\begin{table}[tb!]
  \centering
  \caption{Narrator chain example.}
  \begin{Verbatim}[xleftmargin=1.5cm,fontsize=\relsize{-1},commandchars=\\\{\},codes={\catcode`$=3 \catcode`_=8}]
name:   PN ((MEAN)? PN)*;
nar:    name ((NONE)^3 FAM (NONE)^3 name)*;
pbuh:   BLESS GOD UPONHIM GREET;
nchain: ($s_1=$TOLD $s_2=$nar)+ ((PN|FAM|NONE)^8 pbuh)?
\end{Verbatim}
\resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \toprule 
    \notrrl{القعقاع} & \notrrl{بن} & \notrrl{عمارة} & \notrrl{عن} & \notrrl{جرير} & \notrrl{حدثنا} & \notrrl{سعيد} & \notrrl{بن} & \notrrl{قتيبة} & \notrrl{حدثنا} \\
    \midrule 
    \noarrl{القعقاع} & \noarrl{بن} & \noarrl{عمارة} & \noarrl{عن} & \noarrl{جرير} & \noarrl{حدثنا} & \noarrl{سعيد} & \noarrl{بن} & \noarrl{قتيبة} & \noarrl{حدثنا} \\
    \midrule
    PN    & FAM   & PN    & TOLD  & PN    & TOLD  & PN    & FAM   & PN    & TOLD \\
    \midrule
    name & & name & & name & & name &  & name & \\
    \midrule
    \multicolumn{3}{|c|}{nar} &       & nar   &       & \multicolumn{3}{c|}{nar} &  \\
    \midrule
    \multicolumn{10}{|c|}{nchain}
    \\
    \bottomrule
    \end{tabular}%
}
  \label{tab:nchain}%
\end{table}%
\setcode{standard}

\subsection{Narrator chain case study}
A narrator chain is a sequence of narrators referencing each other. 
%A sample narrator chain is shown in Table~\ref{tab:nchain}. 
The chain includes proper nouns, paternal entities, and referencing entities. 
ANGE uses Arabic morphological analysis, finite state machines, and graph transformations 
to extract named entities and relations including narrator chains~\cite{ZaMaFlairs2012HadithBio}.

\transfalse
MBF \cci{PN} checks the abstract category {\tt Name of Person}. 
MBF \cci{FAM} denotes ``family connector'' and checks the stem gloss ``son''. 
MBF \cci{TOLD} denotes referencing between narrators and checks the disjunction of 
the stems \RL{.hd_t}(spoke to), \RL{`n}(about), \RL{sm`}(heard), \RL{'_hbr}(told), and \RL{'nb-'}(inform). 
MBF \cci{MEAN} checks the stem \RL{`ny}(mean). 
MBFs \cci{BLESS}, \cci{GOD}, \cci{UPONHIM}, and \cci{GREET} check the 
stems \RL{.sll_A}, \RL{Al-ll_ah}, \RL{`ly}, and \RL{sllm}, respectively. 
\transtrue

Table~\ref{tab:nchain} presents the defined MREs. 
MRE {\em name} is one or more \cci{PN} tags optionally followed 
with a \cci{MEAN} tag. 
MRE \cci{nar} denotes narrator which is a complex Arabic name
composed as a sequence of Arabic names (\cci{name}) 
connected with family indicators (\cci{FAM}). 
The \cci{NONE} tags in \cci{nar} allow for unexpected words 
that can occur between names. 
%a complex Arabic name that is constructed
%a \cci{name} tag followed by zero or more 
%sequences of \cci{FAM} tag followed by \cci{name} tag 
%with up to three \cci{NONE} tags. 
MRE \cci{pbuh} denotes a praise phrase often associated with 
the end of a hadith (peace be upon him), 
and is the satisfied by the sequence of
\cci{BLESS}, \cci{GOD}, \cci{UPONHIM}, and \cci{GREET} tags. 
MRE \cci{nchain} denotes narrator chain, 
and is a sequence of narrators (\cci{nar})
separated with \cci{TOLD} tags, and optionally followed
by a \cci{pbuh} tag. 
%nal sequence of up to eight \cci{PN}, \cci{FAM}, or 
%\cci{NONE} tags followed by a \cci{pbuh} tag. 

The first row in Table~\ref{tab:nchain} is an example narrator chain,
the second is the transliteration, the third 
shows the MBF tags. Rows 4, 5, and 6 show the 
matches for \cci{name}, \cci{nar}, and \cci{nchain},
respectively.
%
\framework assigns the symbols $s_1$ and $s_2$ for the 
MRE sub-expressions \cci{TOLD} and \cci{nar}, respectively. 
We define the relation $\langle s_2,s_2',s_1\rangle$ 
to relate sequences of narrators with edges labelled by the tags of \cci{TOLD} where 
$s_2'$ denotes the next match of \cci{nar} in the one or more MRE subexpression.
%The narrators in the example shown in Table~\ref{tab:nchain} are \RL{qtybT bn s`yd}, \RL{jryr}, and \RL{`mArT bn Alq`qA`}. 
%\transfalse
%The edges relating the entities are labeled by the word set \{\RL{.hdd_tnA}, \RL{.hdd_tnA}, \RL{`n}\} which contains all the %matches of the MBF \cci{TOLD}.
%\transtrue

Table~\ref{tab:mbfer} shows that \framework detected almost all the MBF matches 
with 99\% recall and 85\% precision and 
extracted user-defined relations with 98\% recall and 99\% precision.

For brevity, we omit the details of \framework temporal entity extraction, 
genealogy tree, and number normalization case studies, describe them shortly
below and provide a full description in the online Appendix.

\begin{table}[tb!]
  \centering
  \caption{\framework MBF and user-defined relation accuracy }
  \resizebox{0.65\columnwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
     \toprule
     \multirow{2}{*}{Task} & \multicolumn{2}{c|}{MBF accuracy} & \multicolumn{2}{c}{relation accuracy}\\
     & Recall & Precision & Recall & Precision \\
    \midrule
    Narrator chain & 0.99 & 0.85 & 0.99 & 0.98 \\
    Number normalization & 0.99 & 0.99 & 0.97 & 0.95 \\
    Temporal entity & 0.99 & 0.52 & 0.98 & 0.89 \\
    Genealogy tree & 0.99 & 0.75 & 0.81 & 0.96 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:mbfer}%
\end{table}%

\vspace{-1em}
\subsection{Temporal entity extraction}

Temporal entities are text chunks that express temporal information. 
Some represent absolute time such as \RL{Al_hAms mn 'Ab 2010}. 
Others represent relative time such as \RL{b`d _hmsT 'ayAm}, and quantities 
such as \RL{14 ywmA}. 
{\em ATEEMA} presents a temporal entity detection technique for the Arabic language using 
morphological analysis and finite state transducers~\cite{ZaMa2012IJCLATime}. 

Table~\ref{tab:mbfer} shows that \framework detected almost all the MBF matches with 99\% recall, 
however it shows low precision (52\%). 
As for the semantic relation construction, \framework presents a 98\% recall and 89\% precision.

\vspace{-1em}
\subsection{Genealogy tree}

Biblical genealogical lists trace key biblical figures such as Israelite kings and
prophets with family relations. 
The family relations include wife and parenthood. 
A sample genealogical chunk of text is \RL{w wld hArAn lw.tA} 
meaning ``and Haran became the father of Lot''.

GENTREE~\cite{ZaMaHaCicling2012Entity} 
automatically extracts the genealogical family trees using morphology, 
finite state machines, and graph transformations. 
Table~\ref{tab:mbfer} shows that \framework detected 
MBF matches with 99\% recall, and 75\% precision, and
extracted user-defined relations with 81\% recall and 96\% precision.

\newcommand*{\fvtextcolor}[2]{\textcolor{#1}{#2}}
\begin{figure}[tb!]
\centering
  \begin{tabular}{p{5.5cm}p{5.5cm}}
\begin{Verbatim}[fontsize=\relsize{-1},frame=single,label=TMB algorithm,commandchars=\\\[\]] 
cout << \fvtextcolor[red][$s1.text];
if(isHundred) {
  if(current != 0) {
    previous += current;
  }
  current = currentH * \fvtextcolor[red][$s1.number];
  currentH = 0;
  isHundred = false;
  isKey = true;
} else if(current == 0) {
  
  current = \fvtextcolor[red][$s1.number];
  isKey = true;
} else if(!isKey) {

  isKey = true;
  current = current * \fvtextcolor[red][$s1.number];
} else {
  previous += current;
  current = \fvtextcolor[red][$s1.number];
}
\end{Verbatim}
&
\begin{Verbatim}[fontsize=\relsize{-1},frame=single,label=DT algorithm,commandchars=\\\[\]] 
if(isHundred) {
  currentH += \fvtextcolor[red][$s0.number];
} else if(current == 0) {
  current = \fvtextcolor[red][$s0.number];
} else if(isKey) {
  previous += current;
  current = \fvtextcolor[red][$s0.number];
} else {
  current += \fvtextcolor[red][$s0.number]; }
isKey = false;
\end{Verbatim}
\begin{Verbatim}[fontsize=\relsize{-1},frame=single,label=H algorithm,commandchars=\\\[\]] 
isHundred = true;
if(current == 0)  {
  currentH = \fvtextcolor[red][$s2.number];
} else if(!isKey) {
  currentH = current * \fvtextcolor[red][$s2.number];
  current = 0;
} else {
  currentH = \fvtextcolor[red][$s2.number];}
isKey = false;
\end{Verbatim}
\\ 
\end{tabular}
\caption{Actions for TMB, DT, and H MRE expressions.}
\label{fig:numnormalgo}
\end{figure}

\vspace{-1em}
\subsection{Number normalization}
\label{sec:sec:number}
\label{sec:numnorm}

We implemented a number normalization extractor using \framework and 
compared it with {\em NUMNORM}, a 
C++ implementation for number normalization. 
First, we defined the MBFs \cci{DT}, \cci{H}, and \cci{TMB}
to denote (1) digits and tens, (2) hundreds, and (3) 
thousands, millions, and billions, respectively.
The \cci{num} MRE 
\cci{(DT|TMB|H)+} is one or more \cci{DT}, \cci{TMB}, or \cci{H} tags. 
\framework assigns the symbols $s_1$, $s_2$, and $s_3$ 
for the sub-expressions \cci{DT}, \cci{TMB}, and \cci{H}, respectively. 
%The actions associated with the sub-expressions \cci{DT}, \cci{TMB}, and \cci{H} are presented in the appendix.
Figure~\ref{fig:numnormalgo} shows the actions associated with the \cci{DT}, \cci{TMB}, and \cci{H} subexpressions that cumulatively compute the numeric value of the numeric expression match.
The actions use \framework API to access features of the matches such 
as text (\cci{\$s1.text}) and numeric 
value (\cci{\$s1.number}) of literal numbers such as numbers from one to ten.

Table~\ref{tab:mbfer} shows high accuracy in MBF tagging with 99\% recall and 99\% precision, and
high accuracy in relation extraction with 97\% recall and 95\% precision.

\vspace{-1em}
\subsection{Discussion}
\label{subsec:discuss}
\input{discussion}
