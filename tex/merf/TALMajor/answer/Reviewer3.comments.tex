\section*{Reviewer 3 comments} 
\textit{Comments to the Author}



This work is an improvement step in a continuous previous work 
(Saref,MATAr) which leads to a complete project that can be 
used as a tool for other researchers in the field to help 
improving the Arabic language computations and processings.


It depends on the morphological analyser (Saref) results 
to define tags for flexible corpus annotation that is automatic 
but allow the users to correct and fix the tagging. 
In additions it extract entitiy and relational entityies from 
the input document.


\begin{enumerate}[leftmargin=0mm,label=\bfseries CommentR3.\arabic*]
\item \label{Review.3.1}
As a reader, It wasn't very easy to followup with the writing 
style. 
I needed to return back to ``Saref'' and ``MATAr'' papers 
to understand the terminology used.

\answer{
  We added a background section with definitions... 
}

\item \label{Review.3.2}
First, I think we can ask 
``What would a researcher reading this paper need to know?", the answer would be:

1- ``What is new in MERF?''. and this was clear for the reader 
    which is mainly
    a)(entity and relational entity extraction).
    b) defining tag of words based on a synonymic relation

\answer{
  Thank you. 
}

\item \label{Review.3.3}
2- ``How to use MERF?''
(you can have more explanations in your appendex and could have 
``help'' button in your API. This part was also relatively 
clear for the reader.

\answer{
  Thank you. We added a tutorial/video demo... 
}


\item \label{Review.3.4}
3- The most important part is ``how MERF was built 
(methodology)?''. but I found this part was not very clear.

\answer{
  We added a methodology section...
}

\itodo{
  We should the following sections.
    1. \framework methodology: diagram with its description, and the 
    reference to the example +...
    2. \framework supporting tools: the current \framework section
    3. \framework GUI: remain as is

  Look for high level stuff in the (2) and (3) sections that can move
    into (1). 

    When this is done go over the answers to make sure the references
    to answers in the paper are accurate.
}

4-and for sure ``how it was evaluated and good results discussion''. 
which needs more attention.

\answer{
  We added subsection Results.X to discuss the evaluation 
  strategy. 
  We also added extended the discussion section... 
}

\itodo{same answer from Reviewer 1}

The following are some detailed recommendations:

\item \label{Review.3.5}
1-In the introduction, I think you need to omit 
    ``We discuss the importance of the morphological featureotivs supported in 
    MERF terms in Section 3; in brief, morphological preprocessing iskey to 
    Arabic NLP.'' , in page 2 since it will be repeated in page 5.
\answer{
  Done. 
}


\item \label{Review.3.6}
2- in the introduction you illustrated a target example, 
so why don you need the Motivation section. 
What about if combine both.

\answer{
  Thank you. We combine both as motivation and we include that
  as a subsection of the introduction. 
}


\item \label{Review.3.7}
In addition, at page 9 ``the edges and internal nodes are text, 
morphology-based, and word distance based relational entities'' 
is not clear.

\answer{
  We rephrased the sentence in the paper to read as follows. 
  \begin{quote}
The leaf nodes of the match trees are matches to formulae and the internal nodes represent roots to sub-expression matches.
  \end{quote}
}


\item \label{Review.3.8}
please always define your terminology before using it and 
provide examples. suggest the ``formula" was not so clear. 
I suggest using regular grammar instead since you are using 
regular expression and the match tree can be considered as 
parse tree.

\answer{
  We use formula to define predicates that range over 
  the morphological features. 
  We consider the matches of the formulae as tokens/tags (maybe
  we call them tag types.)
  We then use regular expressions (maybe we should use rules). 

  We modify the paper by adopting the suggested terminology: 
  rule for regular expression and parse tree for match tree. 
}

\itodo{
Lets use regular grammar and introduce that with similarity to antler.
MRE becomes a macro maybe a good acronym is morphology based regular grammar rule 
MRGR. 
}

\item \label{Review.3.9}
2- Section ``Existing annotation and entity extraction tools” 
need to be with section 6 or section 6 need to be combined 
with it in as one section but leaving the comparison to 
the analysis section.

\answer{
We merged the content of Subsection 1.1 with the Related Work 
Section~\ref{sec:related}. 
}


\item \label{Review.3.10}
3- Section Background:Morphological analysis, 
the definition explained in page 9 did not match table 1 at page 7.

\answer{
  We modified the definition to indicate that the prefix and suffix 
  and the corresponding tags maybe concatenations of smaller prefix,
  suffix and tags.  
}


\item \label{Review.3.11}
4- Section 5 can be combined with section 2. 
you can introduce the user friendly interface using the 
specified example. 
Section 2 seemed not very imporatnt as stand alone section.

\answer{
  We introduced a new Methodology Section~\ref{sec:methodology} and
  we moved the content of old Section 2 to it. 
}



\item \label{Review.3.12}
a) figure 3 did not make it clear for the reader to 
understand the process. Also, the figure location is far 
away from its explanation.

\answer{
We split Figure 3 (the flow diagram figure) 
into three Figures and provide explanations
for each. 
a. Arabic Text (Consider MBF as input coming from another figure) ----> Tags.
b. Rest of the figure... 
c. RTC + Diff. + Visualizatiin 
}

\itodo{
  Edit the figure and split it  and refer to the (a), (b) and (c) in the paper. 
}


\item \label{Review.3.13}
b) Syn$^k$ is nicely explanied but I did not recognize it as 
your own idea. if this is your idea to use English translation 
as a pivot to extract Arabic synonyms then it needs to have 
more focus because I think this is new.

\answer{
Up to our knowledge, we introduced the idea. 
We claim that now as a novelty and a contribution in the introduction
and we rephrase the beginning of Section~\ref{sec:synk}. 
}

\item \label{Review.3.14}
what is $2^{gloss}$ and $2^{s}$?

\answer{
The notation $2^s$ denotes the power set of set $s$ which is 
the set of all subsets of $s$. 
We clarify that in the paper. 
}


\item \label{Review.3.15}
please to make it easier for the reader to followup, 
let the example in figure 4 also be defined as the formula 
variables. w,{u},..so on

\answer{
We modified the paragraph explaining the example to refer
to the formula variables. 
}


\item \label{Review.3.16}
c) In MRE section,page 10, $k <=7$ why? 

\answer{
  We answer this by introducing the following in the paper.
  \begin{quote}
  We limit $k$ to a maximum of $7$ since we practically noticed that 
  (1) values above $7$ introduce significant semantic noise and
  (2) the computation is expensive without a bound. 
  \end{quote} 
}

\item \label{Review.3.17}
also, A belongs-to F , A is a morphological feature, 
then how CF belongs-to A. A is defined as one item not a set.

\answer{
This is a mistake. We corrected that in the paper to 
read as follows... 
}

\itodo{
  Ameen please find where you fixed this and say it here. 
}


\item \label{Review.3.18}
MRT definition is not clear.

\answer{
We clarify the definition of MRE in the paper. 
}

\itodo{
  Clarify the definition. 
}


\item \label{Review.3.19}
"MRE" and "user defined relations and actions " sections 
were hard to be understood.

\answer{
We rewrote both sections... 
}

\itodo{
  Figure our something to do/say.
}

\item \label{Review.3.20}
MREF simulator: please provide examples soon after the 
definitions to make it easier for the reader. 
The reader can be lost so fast within the variable definitions. 

\answer{
We provide running examples with the definitions....
}
\itodo{
  Color them and say where they are. 
}


\item \label{Review.3.21}
for each word you are computing all the formulas values, 
so the complexity is O(text length X number of formulas), 
what is the range of number of formulas ?

\answer{
  The complexity is the number of morphological solutions 
  times the number of user defined formulae. 
  We provide no limit on the number of user defined formulae.
  Practically, in our case studies we did not need to defined
  more than 10 formulae per case study. 
  We include this discussion in the Results Section of the paper. 
}

\itodo{
  Add that to the Results. 
}


\item \label{Review.3.22}
you are using R as tuple defining relation, 
then bellow you used it as set of feature vectors for the text. 
This is confusing the reader. 
I needed to read the definition in MATAr paper to understand 
what R is.

\answer{
  We cleaned the definitions... 
}

\itodo{
  Ameen: highlight the changes and refer to them please. 
}


\item \label{Review.3.23}
d) In general, what was the data structure used? 
how would the annotated text be saved?

\answer{
  We used XXX data structures... 
}

\itodo{
  Ameen: JSON add that as a paragraph in the methodology section. 
}


\item \label{Review.3.24}
6- At section Results, comparing the development time 
did not strength the work. 
This is my first time to see researchers compare their 
algorithm in term of line of code and developing time. 
I would suggest comparing the algorithm complexity 
(linear , exponential, etc), and comparing how fast a user can 
accomplish similar jobs in the different applications. 
This can be done by assigning similar corpus annotation job 
and information extraction while computing time needed by 
the user 
``as a survey like comparison for qualitative evaluation''. 
you can search for ``user interface system evaluation''. 
There is heuristic evaluation where you give weight for job 
complexity and record time needed to complete it.

\answer{
Our evaluation is a survey based evaluation.
Users used \framework to define their \framework based annotators. 
Users also built finite state transducer based annotators to 
perform the same tasks on the same corpora. 
We measured and reported the time needed by both. 

We also performed manual annotation to construct the correctness 
benchmark.
We did that by correcting the automated annotations of both annotators
for the benchmark corpora. 
That alone took much more time than the time it took to develop and run
the \framework annotators. 

We clarify that in the beginning of the Results Section ~\ref{sec:results}.
}

\itodo{
  Ameen: lets add the above to the section. 
}

\item \label{Review.3.24}
Also, comparison with previous work done by (Zaraket ) 
in different vesion seems like comparing with your previous work 
only and not with others. 
It would be nice to compare others work too because user 
interface could be introduced in different view.

\answer{
  \framework targets entity and 
  relational entity extraction. 
  We are not aware and do not have access to other 
  previous work that performs morphology based relational entity 
  extraction for Arabic.
  We are open to perform such experiments in case the reviewers 
  can point us to such accessible tools. 
}

\item \label{Review.3.24}
What is the data size of your evaluation? 
I couldn't find how presion and recal was counted. 
you mentioned books but did you covered all the text in those 
books ? and how long was it? you can define it in term of 
number of relation extracted , tags added, expesion processed, 
sentences,..etc

\answer{
  We include more information about the data and 
  the evaluation process... 
}

\itodo{
  Answer.. 
}


\item \label{Review.3.24}
7- At section Discussion, I strongly urge you to discuss 
WHY the precision and recall have these values.

\answer{
We extend the discussion section to discuss ... 
}

\item \label{Review.3.25}
Clarity:  
It was not very clear how the work was done 
``Methodology part''. 
Section 4 ``MERF'' please add ``Methedology''.

\answer{
  We added a methodology section. 
}


\item \label{Review.3.25}
Having ``Existing annotation and entity extraction tools'' 
under unspecified section is misleading the reader. 
unnumbered subsections was misleading me as a reader. 

\answer{
  Fixed. CLS file was not compatible with arabtex package. 
}

\item \label{Review.3.25}
table 1 at page 7 while referring to it only at page 9. 
I think it needs to be closer to where it was mentioned in the text. 

\answer{
  Done.
}

\itodo{
  Make sure this is moved. 
}

\item \label{Review.3.25}
``MERF regular expressions support operators such as concatenation, zero or one, zero or more, one or more, up to $M$” what is M? 
it could be for example 0<=M<=|chunk| 


\answer{
  We clarify the 
  notation ``up to $M$'' in the paper to be as follows. 
  \begin{quote}
   up to $M$ repititions where $M$ is 
   a non-zero positive integer 
  \end{quote}
}


\item \label{Review.3.25}
Correctness: 
1- in the abstract, the sentence ``These techniques and 
tools require expertise in linguistics and programming and 
lack support of Arabic morphological analysis which is key to 
process Arabic text''
gives the meaning that ``it require lack support'' so I think it 
needs to be reformulated 

\answer{
  We rephrased the sentence.
}


\item \label{Review.3.25}
2-``defines tag types and''.// unify the font here and 
many other places in the paper. 
for example look at words(,correct,commercial)

\answer{
  We use italics to emphasize important 
  concepts that we introduce for the first time. 
  We reduced the use of emphasize in the paper. 
}

\item \label{Review.3.25}
Originality:  
This work will contribute in facilitating Arabic corpus 
annotation, if it was made available for other researchers in 
the field to access and use it, although there is no 
referencing to where it can be accessed. 
this is an improvement of a per-existing work ``MATAr: Morphology-based Tagger for Arabic''

\answer{
\framework is open source and available online. We modified the
contributions in the Introduction Section to indicate that. 
}


\end{enumerate}
