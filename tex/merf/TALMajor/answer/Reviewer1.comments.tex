\section*{Reviewer 1 comments}
\textit{Comments to the Author}\\


The paper describes a tool (MERF) for entity and relation 
extraction in Arabic text.
The tool allows to describe regular expressions which atomic 
elements are morphological descriptions of Arabic words.
The regular expressions are compiled as finite state 
automata that are used for pattern matching.

MERF uses a morphological analyzer for Arabic Sarf that 
is itself based on finite state transducers.
MERF proposes several GUI to edit rules and analyze results.
The paper is clearly written. 

\noindent\answer{
  Thank you. 
}

\begin{enumerate}[leftmargin=0mm,label=\bfseries CommentR1.\arabic*]

  \item \label{Review1.1} 
The authors chose to use formal notation to describe the 
objects manipulated by the software. 
This make things rigouros but the notations are sometimes heavy.

\answer{
  We added a methodology section that precedes the formal 
    notation. 
  We also embedded a running example along with the formal
    notations. 
}


\item \label{Review1.2} 
I did not understand what is the exact status of the predicate 
isA and how it is used, the authors should be a bit 
 clearer about that.

\answer{
  We clarified the \cci{isA} predicate in Section~\ref{subsec:grammar}.
  Basically, isA is used to detect the `exact match' 
  between a user-chosen value for a morphological feature and the 
  corresponding feature value in a morphological solution of a word.

  We also replaced a mistake that referenced the \cci{isA} predicate
  in Figure~\ref{fig:intromotiv} by the correct predicate \cci{isSyn}. 
}

\item \label{Review1.3} 
The system is evaluated and compared to four other systems for 
tasks of Information Extraction.
the data used for the evaluation is not clearly described, 
did you have a manual gold annotation of all the data ?

\answer{
  We added description of the case studies in the Results Section~\ref{sec:results}. 
  The reference chunks are manually created gold annotations.
  Manual annotators inspected the outcome of the tools and provided 
corrections where tools made mistakes. 
The corrections form the manual gold annotation that we
compared against. 
We indicate that in the Results Section. 
}


\item \label{Review1.4} 
what is the size of the evaluation data ? 
did you have a train and a test data set ?

\answer{
  We added a column to Table~\ref{tab:results} to refer to the size 
  of the annotated data for each case study and we discussed the numbers
  in the Results Section~\ref{sec:results}. 
  \framework does not require training. 
  It is an interactive tool where the user provides the formulae, expressions,
    relation tuples and the associated code. 
  The user inspects the results and refines the model accordingly. 
  The testing set is available with the tool. 
}

\item \label{Review1.5} 
how did you compute the time for the implementation of rules 
with the other system (one of the tasks needed 2 month of work 
!! is it really 2 acutal months of work or is it the period of 
time during which the development has been done)
more should be said about the evaluation methodology.

\answer{
  We clarify this issue in the Results Section~\ref{sec:results}.
A research assistant 6 hours course work and 14 hours 
teaching duties performed the task in two months. 
}

\item \label{Review1.5}
you say nothing about the availability of the system, 
is it distributed ? free ? what kind of licence ?

\answer{
\framework is open source and available online. We modified the
contributions in the Introduction Section~\ref{sec:introduction} to indicate that. 
}

\end{enumerate}
